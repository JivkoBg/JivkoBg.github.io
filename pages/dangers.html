<!DOCTYPE html>
<html lang="bg">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Опасности</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <header>
        <h1>Опасности от ИИ в автомобилите</h1>
        <nav>
            <ul>
                <li><a href="../index.html">Начало</a></li>
                <li><a href="introduction.html">Въведение</a></li>
                <li><a href="applications.html">Приложение</a></li>
                <li><a href="advantages.html">Предимства</a></li>
                <li><a href="future.html">Бъдеще</a></li>
                <li><a href="conclusion.html">Заключение</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <h2>Киберсигурност</h2>
        <p>С навлизането на изкуствения интелект (ИИ) в ключови сектори като здравеопазване, транспорт, финанси и индустрия, се повишава не само ефективността на процесите, но и обемът от данни, който ежедневно се събира, анализира и съхранява. Изкуственият интелект обработва огромно количество данни, което го прави уязвим за хакерски атаки и злонамерен достъп.

            Тези данни често съдържат чувствителна информация – лични профили, търговски тайни, производствени параметри или стратегически модели. Ако системата не е адекватно защитена, дори една слабост в алгоритъма или в свързаната инфраструктура може да бъде използвана за компрометиране на целия процес. Атаките срещу ИИ могат да варират – от манипулиране на входните данни, което води до погрешни изводи, до пълен контрол над системата чрез зловреден софтуер.
            
            Един от сериозните рискове е така наречената „атака чрез подправени данни“ (data poisoning), при която хакери въвеждат изкуствено създадена информация с цел да обучат модела да взема грешни решения. Това може да има сериозни последици, особено в системи, отговарящи за безопасност, здраве или финансови транзакции.
            
            Затова е от изключителна важност разработчиците на ИИ да интегрират надеждни мерки за киберсигурност още от етапа на проектиране. Необходимо е прилагането на криптографска защита, строги политики за достъп, непрекъснат мониторинг и редовно тестване на устойчивостта срещу възможни атаки.
            
            Използването на изкуствен интелект предлага безспорни ползи, но те трябва да вървят ръка за ръка с отговорност и осъзнаване на потенциалните рискове. Сигурността на данните и устойчивостта на системите срещу атаки са ключови фактори за изграждането на доверие и стабилност в дигиталната ера.</p>

        <h2>Отговорност при злополуки</h2>
        <p>Развитието на автономните превозни средства променя из основи начина, по който се придвижваме, и обещава революция в транспорта – с по-висока безопасност, по-малко задръствания и намалено човешко участие. Но с тази технологична трансформация възникват и редица въпроси, свързани не само с техниката, но и с правото и етиката. Един от най-значимите и все още неразрешени въпроси е: кой носи отговорност при инцидент, причинен от автономен автомобил?

            Автономните автомобили поставят въпроса за юридическата отговорност при инциденти, тъй като те вземат решения самостоятелно, без пряка човешка намеса. В класическата ситуация при ПТП, отговорността се носи от водача – но когато няма активен водач, а системата за изкуствен интелект управлява автомобила, се размива границата между "оператор" и "автоматизирана система".
            
            Няколко възможни сценария се обсъждат в юридическите и технологичните среди. Един вариант е производителят на автомобила или софтуера да носи отговорност, ако се докаже, че е допусната техническа грешка или дефект в алгоритъма. Друг подход предвижда разпределена отговорност между производителя, собственика на автомобила и дори доставчиците на навигационни и свързани услуги.
            
            Особено сложни са случаите, в които ИИ взема решения в условия на морална дилема – например, при необходимост да избере между защита на пътниците и избягване на пешеходец. Кой би носил отговорност в подобна ситуация – алгоритъмът, програмистите, компанията или никой?
            
            Законодателните рамки в повечето страни все още догонват темповете на развитие на автономните технологии. Нужни са нови регулации, ясно дефинирани отговорности и международни стандарти, които да гарантират справедливост, прозрачност и защита както на потребителите, така и на производителите.</p>

        <h2>Етични дилеми</h2>
        <p>С нарастващото присъствие на изкуствения интелект в ежедневието, особено в критични сфери като здравеопазване, транспорт и сигурност, нарастват и въпросите, свързани с етиката на неговите действия. Макар че ИИ притежава изключителна способност да обработва данни и да взема бързи решения, възникват сериозни притеснения относно начина, по който той реагира в ситуации, свързани с риск и морални избори.

            Решенията, които AI взема при рискови ситуации, остават предмет на дебат, тъй като алгоритмите не притежават човешка интуиция, емоции или морални ценности. Когато ИИ бъде поставен пред избор – например да защити пътниците в автономен автомобил за сметка на пешеходец, или да спре лечението на пациент въз основа на статистически прогнози – въпросът не е само технически, а дълбоко етичен.
            
            Кой определя какво е „правилно“ решение в подобен контекст? Програмистите, разработващи алгоритмите? Компаниите, които ги внедряват? Или обществото, което трябва да определи моралната рамка, в която ИИ ще функционира? За разлика от хората, ИИ не може да носи морална отговорност – той действа според предварително зададени правила и данни. Но именно тези правила трябва да бъдат внимателно обмислени и широко обсъдени.
            
            Дебатът се усложнява от факта, че различните култури и общества имат различни ценности и приоритети. Един и същ ИИ, работещ в различни страни, може да се изправи пред напълно противоположни очаквания. Това налага създаване на международни етични стандарти и механизми за прозрачност и отчетност при използването на ИИ в рискови ситуации.
            
            Изкуственият интелект има потенциала да взема решения с точност и скорост, непостижими за човека – но дали тези решения са справедливи, морални и приемливи за обществото, остава открит въпрос. Ето защо е от съществено значение не само какво може да направи ИИ, а какво трябва да направи.
            
            </p>
    </main>

    <footer>
        <p>&copy; 2025 Живко Беджев | Всички права запазени</p>
    </footer>
    <style>
  body {
    font-family: Arial, sans-serif;
    background-image: url("../artificial-intelligence-in-self-driving-cars-Cover-100620211132.webp");
    background-position: center;
    background-repeat: no-repeat;
    background-size: cover;
    background-attachment: fixed;
    margin: 0;
    padding: 0;
    color: white;
}


header {
    background-color: rgba(0, 0, 0, 0.7);
    padding: 15px;
    text-align: center;
    color: white;
}

nav ul {
    list-style: none;
    padding: 0;
    text-align: center;
}

nav ul li {
    display: inline;
    margin: 0 15px;
}

nav ul li a {
    color: white;
    text-decoration: none;
    font-weight: bold;
    background: rgba(0, 0, 0, 0.6);
    padding: 8px;
    border-radius: 5px;
}

nav ul li a:hover {
    background: rgba(255, 255, 255, 0.3);
}

main {
    text-align: center;
    padding: 20px;
    max-width: 800px;
    margin: auto;
    background: rgba(0, 0, 0, 0.5);
    border-radius: 10px;
}

footer {
    background: rgba(0, 0, 0, 0.7);
    color: white;
    text-align: center;
    padding: 10px;
    margin-top: 20px;
}
    </style>
</body>
</html>
